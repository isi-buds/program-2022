---
title: "UC Irvine  ISI-BUDS Day 14"
author: "Zhaoxia Yu"
date: "7/29/2022"
geometry: "left=0.5cm,right=2.5cm,top=1cm,bottom=1cm"
output: 
  beamer_presentation:
    theme: "Goettingen"
    colortheme: "whale"
    fonttheme: "structurebold"
    slide_level: 2
header-includes:
  \setbeamertemplate{footline}[page number]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Study Goals
* LM, LME, GLM, and GLMM
\vspace{0.3cm}
* LME Examples: Example 2
\vspace{0.3cm}
* LME Examples: Example 3
\vspace{0.3cm}
* Generalized Linear Mixed-Effects Model (GLMM)
\vspace{0.3cm}
* The slides are based on my published work:
https://doi.org/10.1016/j.neuron.2021.10.030
https://yu-zhaoxia.github.io/MM_in_Neuroscience/

# LM, LME, GLM, and GLMM

## LM and its Matrix Form
* The model
$$Y_i = \beta_0 + x_{i1} \times \beta_1+ … + x_{ip} \times \beta_p + \epsilon_i , i =1, …, n$$
* Assumptions
    + $E(\epsilon_i)=0$, which is equivalent to $E(Y_i|X_i)=\beta_0 + x_{i1} \times \beta_1+ … + x_{ip} \times \beta_p$
    + $Var(\epsilon_i)=\sigma^2$, which is equivalent to $Var(Y_i|X_i)=\sigma^2$.
    + $(\epsilon_1, \cdots, \epsilon_n)$ are mutually independent
\vspace{0.3cm}
* The equivalent matrix form is \textcolor{red}{$Y=X\beta + \mathbf \epsilon$}, where the random errors have a zero mean and a common variance, and are independent with each other

## LM and LME
* The components of LM: \textcolor{red}{$Y=X\beta + \mathbf\epsilon$}
    + a linear predictor $X\beta$
    + random errors $\mathbf \epsilon$ are independent, have a zero mean and a constant variance. 
    + $\mathbf \epsilon \sim N(0, \sigma^2 \mathbf I)$ is used for deriving t- and F-tests. Typically this assumption is very critical as long as the sample size is not too small 
* The components of LME: \textcolor{red}{$Y=X\beta + Z\mathbf u + \mathbf\epsilon$}
    + fixed-effects: a linear predictor $X\beta$
    + random-effects: $Z\mathbf u$, where $\mathbf u\sim N(0, G)$. E.g., $G=\sigma_b^2 \mathbf I$.
    + random errors: $\mathbf \epsilon \sim N(0, \sigma^2 \mathbf I)$ 

## GLM
* The components of GLM: 
    + a linear predictor $X\beta$
    + a link function to connect $E(Y|X)$ and $X\beta$: \textcolor{red}{$g(E(Y|X))=X\beta$}
    + a distribution for $Y$
* The components of GLMM: 
    + fixed-effects: a linear predictor $X\beta$
    + random-effects: $Z\mathbf u$, where $\mathbf u\sim N(0, G)$. E.g., $G=\sigma_b^2 \mathbf I$.
    + a link function to connect $E(Y|X, \mathbf u)$ and $X\beta$: \textcolor{red}{$g(E(Y|X, \mathbf u))=X\beta + Z\mathbf u$}
    + a distribution for $Y$


# LME Examples: Exmaple 2
## LME Examples: Exmaple 2
* Research question: determine how in vivo calcium (Ca++) activity of PV cells (measured longitudinally by the genetically encoded Ca++ indicator GCaMP6s) changes over time after ketamine treatment
* Study:  Ca++ event frequencies were measured at 24h, 48h, 72h, and 1 week after ketamine treatment in four mice
* Want to compare Ca++ event frequency at 24h to the other three time points. 
* In total, Ca++ event frequencies of 1,724 neurons were measured. 

## Example 2: Data
```{r, message=FALSE, warning=FALSE}
library(nlme)
library(lme4)
library(lmerTest)
Ex2=read.csv("https://www.ics.uci.edu/~zhaoxia/Data/BeyondTandANOVA/Example2.txt", head=T)
Ex2$treatment_idx=Ex2$treatment_idx-4
Ex2$treatment_idx=as.factor(Ex2$treatment_idx)
### covert the variable of mouse IDs to a factor
Ex2$midx=as.factor(Ex2$midx)
```

## Example 2: Wrong analysis
```{r}
lm.obj=lm(res~treatment_idx, data=Ex2)
summary(lm.obj)$coefficients
```

## Example 2: Wrong analysis
* The LM (including ANOVA, t-test) analysis results indicate 
    + significantly reduced Ca++ activity at 48h relative to 24h with $p=4.8\times 10^{-6}$         
    + significantly increased Ca++ activity at 1week compared to 24h with $p=2.4\times 10^{-3}$
    + However, if we account for repeated measures due to cells clustered in mice using LME, the changes are no longer significant

## Example 2: LME
```{r}
lmer.obj=lmerTest::lmer(res~treatment_idx+(1|midx), data= Ex2, REML="FALSE")
summary(lmer.obj)$coefficients
```

## Example 2: LM vs LME
Estimated changes of Ca+ event frequency (the baseline is 24h after treatment)
```{=latex}
\begin{tabular}{cccc}
& 48h & 72h & 1wk \\ \hline
LM est	 &-0.078 $\pm$.017 &	0.009$\pm$0.017	& 0.050$\pm$0.016\\	
LM p &	$4.8\times 10^{-6}$	& 0.595	& $2.4\times 10^{-3}$	\\
LME est	& -0.017$\pm$0.017	& 0.009$\pm$0.017	& 0.029$\pm$0.017	\\
LME p	& 0.311	& 0.573	& 0.076 
\end{tabular}
```

## Pooling data naively is not a good idea 
```{r S2, out.width="72%", fig.cap="The boxplots of Ca++ event frequencies measured at four time points. (A) Boxplot of Ca++ event frequencies using the pooled neurons from four mice. (B) boxplots of Ca++ event frequencies stratified by individual mice.", echo=FALSE}
knitr::include_graphics("Fig2S.jpg")
```

## Pooling data naively is not a good idea 
* Consider the change in Ca++ activities from 24h to 48h
* Pooled data from all mice:
    + The box plots suggest reduction in Ca++ activities
* Individual mice data:
    + The box plots of Mouse 2 suggest a noticeable reduction
    + However, there was almost no change in Mouse 1
    + Mouse 3 and Mouse 4 might suggest small increases, rather than decreases

## Pooling data naively is not a good idea 
* Why do the pooled data follow the pattern of Mouse 2?
```{r, message=FALSE, include=FALSE}
# the number of cells in each animal-time combination
table(Ex2$midx, Ex2$treatment_idx)
# compute the percent of cells contributed by each mouse
rowSums(table(Ex2$midx, Ex2$treatment_idx))/1724
```
```{=latex}
\begin{tabular}{cccccc}
& 24h & 48h & 72h & 1wk & Total\\ \hline
Mouse 1 & 81 & 254 &	88	& 43	& 466(27$\%$)\\
Mouse 2	& 206	& 101	& 210	& 222	& 739 (43$\%$)\\
Mouse 3	& 33	& 18	& 51	& 207	& 309 (18$\%$)\\
Mouse 4	& 63	& 52	& 58	& 37	& 210 (12$\%$)\\
Total	& 383	& 425	& 407	& 509	& 1,724 (100$\%$) 
\end{tabular}
```
* Mouse 2 contributed 43% cells!
\vspace{2cm}

## Remark: on the minimum number of levels for using random-effects
* In Example 2, the number of levels in the random-effects variable is four, as there are four mice. 
\vspace{1cm}
* According to Gelman and Hill 2006, it does not hurt to use random-effects in this situation. 
\vspace{1cm}
* There is no unique answer on the minimum number of levels for using random-effects. 

## Remark: on the minimum number of levels for using random-effects
* An alternative is to include the animal ID variable as factor with fixed animal effects.
* Neither of two approaches is the same as fitting an LM to the pooled cells naively. 
* In a more extreme case, for an experiment using only two monkeys for example, 
    + naively pooling data (such as neurons) is NOT recommended. 
    + a more appropriate approach is to analyze the animals separately and then check whether the results from the two animals are consistent


# LME Examples: Example 3 
## Example 3: Data Structure
* Ca++ event integrated amplitudes are compared between baseline and 24h after ketamine treatment. 
* 622 cells were sampled from 11 mice
* each cell was measured twice (baseline and after ketamine treatment)
* correlation arises from both cells and animals, which creates a three-level structure:          
    + measurements within cells and cells within animals. 
```{R, eval = FALSE}
library(nlme)
library(lme4)
Ex3=read.csv("https://www.ics.uci.edu/~zhaoxia/Data/BeyondTandANOVA/Example3.txt", head=T)
```
\vspace{1cm}

## Example 3: LM vs LME
```{R, eval = FALSE,}
#### wrong analysis: using the linear model
summary(lm(res~treatment, data=Ex3[!is.na(Ex3$res),])) #0.0036
#### wrong anlaysis using t tests (paired or unpaired)
t.test(Ex3[Ex3$treatment==1,"res"], Ex3[Ex3$treatment==2,"res"], var.eq=T)
t.test(Ex3[Ex3$treatment==1,"res"], Ex3[Ex3$treatment==2,"res"])
t.test(Ex3[Ex3$treatment==1,"res"], Ex3[Ex3$treatment==2,"res"], paired=T)

#LME
lme.obj1=lme(res~ treatment, random =~1| midx/cidx, data= Ex3[!is.na(Ex3$res),], method="ML")
summary(lme.obj1)
```

## Example 3: LM vs LME
* LME and LM produce similar estimates for the fix-effects coefficients
* the standard error of the LM is larger; the p-value based on LME is smaller (0.0036 for LM vs 0.0001 for LME). 
* In this example, since the two measures from each cell are positively correlated, the variance of the differences is smaller when treating the data as paired rather than independent. 
* As a result, LME produces a smaller p-value
* \textcolor{red}{Rigorous statistical analysis is not a hunt for the smallest
p value (commonly known as p-hacking or significance chasing)}

##   
```{R S3, echo=FALSE, out.width="100%", fig.cap="(Left) the scatter plot of Ca++ event integrated amplitude at baseline vs 24h after treatment for the neurons from four mice (labeled as 1, 2, 3 and 4) indicates that the baseline and after-treatment measures are positively correlated. (Right) boxplot of the baseline and after-treatment correlations of the 11 mice."}
knitr::include_graphics("Fig3S.png")
```

## A note on “nested” random effects
* When specifying the nested random effects, we used “random =~1| midx/cidx”. 
* This leads to random effects at two levels: the mouse level and the cells-within-mouse level. 
* This specification is important if same cell IDs might appear in different mice. 
* When each cell has its unique ID, just like “cidx” variable in Example 3, it does not matter and “random =list(midx=~1, cidx=~1)” leads to exactly the same model.

## A note on “nested” random effects
```{R, eval = FALSE}
### to verify that the cell IDs are indeed unique
length(unique(Ex3$cidx))
#lme.obj2 is the same as lme.obj
lme.obj2=lme(res~ treatment, random =list(midx=~1, cidx=~1), data=Ex3[!is.na(Ex3$res),], method="ML")
summary(lme.obj2)
```

## On models with more random effects
* The above LME model only involves random intercepts. 
* There might be random effects due to multiple sources.
* A model with more random-effects might be a better choice.
* Visualization is a useful exploratory tool to help identify an appropriate model.

## On models with more random effects
```{R S4, echo=FALSE, out.width="90%", fig.cap="Ca++ event integrated amplitudes at baseline vs 24h after treatment for the neurons from four mice (labeled as A, B, C and D) with each dot representing a neuron. The four plots on the left are “spaghetti” plots of the four animals with each line representing the values at baseline and 24h after treatment for a neuron; the four plots on the right report the before-after scatter plots (with fitted straight lines using a simple linear regression)."}
knitr::include_graphics("Fig4S.png")
```

## On models with more random effects
* Tests can be used to compare models with different random effects
    + Need to be careful. See 6.4 of https://yu-zhaoxia.github.io/MM_in_Neuroscience/
* For exmaple 3, the model I chose have the following random-effects:

"random=list(midx=~1, cidx=~treatment)"

* It improves lme.obj1 substantially. 

* Adding more random-effects does not lead to much improvement

# GLMM 
## Generalized Linear Mixed-Effects Model (GLMM)
* The components of aGLMM: 
    + fixed-effects: a linear predictor $X\beta$
    + random-effects: $Z\mathbf u$, where $\mathbf u\sim N(0, G)$. E.g., $G=\sigma_b^2 \mathbf I$.
    + a link function to connect $E(Y|X, \mathbf u)$ and $X\beta$: $$g(E(Y|X, \mathbf u))=X\beta + Z\mathbf u$$
    + a distribution for $Y$
    
## GLMM Examples: A Simulated Data Set
* The simulation used parameters estimated from real data
* Eight mice were trained to do task
* The behavior outcome is whether the animals make the correct predictions
    + 512 trials in total: 216 correct trials, 296 wront trials
* Mean neuronal activity levels (dF/F) were recorded for each trial
* We would like to model behaviors using neuronal data (decoding)

## Use lme4::glmer to fit a GLMM 
```{R, warning=FALSE}
library(lme4) 
library(pbkrtest)
waterlick=read.table("https://www.ics.uci.edu/~zhaoxia/Data/BeyondTandANOVA/waterlick_sim.txt", head=T)
summary(waterlick)
#change the mouseID to a factor
waterlick[,1]=as.factor(waterlick[,1])
```
\vspace{1cm}

## Use lme4::glmer to fit a GLMM 
```{R}
obj.glmm=glmer(lick~dff+(1|mouseID),
data=waterlick,family="binomial")
#summary(obj.glmm)
#compute increase in odds and a 95% CI
exp(c(0.06235, 0.06235-1.96*0.01986, 0.06235+1.96*0.01986))-1
```

## Interpret GLMM results
* The estimate of odd is 6.4% increase and a 95% confidence interval is 2.3% to 10.7%
* The interpretation of the fixed effects for GLMM is complicated by both 
    + the random effects and
    + non-linear link functions
* Among typical mice, the odds of making correct licks increased by 6.4% (95% C.I.: 2.4%-10.7%) with one unit increase in dF/F.

## LRT test
* Likelihood ratio test can be done by comparing the model with and the model without the ``dff" variance (neuronal activity). Large-sample approximation is used.
```{R, eval = FALSE}
#fit a smaller model, the model with the dff variable removed
obj.glmm.smaller=glmer(lick~(1|mouseID),
data=waterlick,family="binomial")
#use the anova function to compare the likelihoods of the two models
anova(obj.glmm, obj.glmm.smaller)
#alternatively, one can use the “drop1” function to test the effect of dfff
drop1(obj.glmm, test="Chisq")
```

## Improve accuracy of p-values
* The large-sample approximations in GLMM might not be accurate 
* We show how to conduct a parametric bootstrap test
```{R, eval = FALSE, cache=TRUE, message=FALSE, warning=FALSE}
#The code might take a few minutes
PBmodcomp(obj.glmm, obj.glmm.smaller)
```
* By default, 1000 samples were generated to obtain an empirical null distribution of the likelihood ratio statistic

## Convergence Issues
* GLMM is harder to converge than LME. 
    + Increase the number of iterations
    + Switch to a different numerical maximization methods
    + Modify models such as eliminate some random effects

https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html

https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html

https://m-clark.github.io/posts/2020-03-16-convergence/

## Convergence Issues
* Consider more robust methods such generalized equating equation (GEE)
\vspace{1cm}
* Oftentimes, Bayesian approaches are easier to converge
