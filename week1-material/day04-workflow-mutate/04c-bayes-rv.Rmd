---
title: "Bayes' Rule for Random Variables"
subtitle: "ISI-BUDS"
author: "Dr. Mine Dogucu"
date: "2022-07-14"
output: 
  xaringan::moon_reader:
    css: ["slide-style.css"]
    lib_dir: libs
    seal: false
    nature:
      ratio: 16:9
      highlightStyle: "pygments"
      highlightLines: true
      highlightLanguage: "r"


---

class: title-slide

<br>
<br>
# `r rmarkdown::metadata$title`
## `r rmarkdown::metadata$author`
### `r rmarkdown::metadata$subtitle`
### `r rmarkdown::metadata$date`

---

class: middle center

.font75[Review: Bayes' Rule for Events]

---

class: middle

## Spam email

Priya, a data science student, notices that her college's email server is using a faulty spam filter.  Taking matters into her own hands, Priya decides to build her own spam filter.  As a first step, she manually examines all emails she received during the previous month and determines that 40% of these were spam.  

---

class: middle

## Prior 

Let event B represent an event of an email being spam.

$P(B) = 0.40$

If Priya was to act on this prior what should she do about incoming emails?

---

class: middle

## A possible solution

Since most email is non-spam, sort all emails into the inbox.  

This filter would certainly solve the problem of losing non-spam email in the spam folder, but at the cost of making a mess in Priya's inbox.  

---

class: middle

## Data

Priya realizes that some emails are written in all capital letters ("all caps") and decides to look at some data. In her one-month email collection, 20% of spam but only 5% of non-spam emails used all caps. 

--

Using notation:

$P(A|B) = 0.20$

$P(A|B^c) = 0.05$

---

class: middle

<div align = "center">

```{r priya, echo = FALSE, fig.align='center', message = FALSE, warning = FALSE}
library(DiagrammeR)
library(tidyverse)
grViz(diagram = "
    digraph {
        # graph aesthetics
        graph [ranksep = 0.5]
        # node definitions with substituted label text
        1 [label = 'Prior: \n Only 40% of emails are spam.']
        2 [label = 'Data: \n All caps is more common among spam.']
        3 [label = 'Posterior: \n Is the email spam or not?!']
        # edge definitions with the node IDs
        1 -> 3
        2 -> 3
    }
")
```

</div>

---

class: middle

Which of the following best describes your posterior understanding of whether the email is spam?


a. The chance that this email is spam drops from 40% to 20%.  After all, the subject line might indicate that the email was sent by an excited professor that's offering Priya an automatic "A" in their course!  
b. The chance that this email is spam jumps from 40% to roughly 70%.  Though using all caps is more common among spam emails, let's not forget that only 40% of Priya's emails are spam.  
c. The chance that this email is spam jumps from 40% to roughly 95%.  Given that so few non-spam emails use all caps, this email is almost certainly spam.

---

class: middle

## The prior model

<div align="center">

| event       | $B$ | $B^c$ | Total |
|-------------|-----|-------|-------|
| probability | 0.4 | 0.6   | 1     |



---

class: middle

## Likelihood

Looking at the conditional probabilities

$P(A|B) = 0.20$

$P(A|B^c) = 0.05$

we can conclude that all caps is more common among spam emails than non-spam emails. Thus, the email is more **likely** to be spam. 

Consider likelihoods $L(.|A)$:

$L(B|A) := P(A|B)$ and $L(B^c|A) := P(A|B^c)$

---

class: middle

### Probability vs likelihood   

When $B$ is known, the __conditional probability function__ $P(\cdot | B)$ allows us to compare the probabilities of an unknown event, $A$ or $A^c$, occurring with $B$: 

$$P(A|B) \; \text{ vs } \; P(A^c|B) \; .$$  

When $A$ is known, the __likelihood function__ $L( \cdot | A) := P(A | \cdot)$ allows us to compare the likelihoods of different unknown scenarios, $B$ or $B^c$, producing data $A$:

$$L(B|A) \; \text{ vs } \; L(B^c|A) \; .$$
Thus the likelihood function provides the tool we need to evaluate the relative compatibility of events $B$ or $B^c$ with data $A$. 

---

class: middle

### The posterior model

$P(B|A) = \frac{P(A\cap B)}{P(A)}$

--

$P(B|A) = \frac{P(B)P(A|B)}{P(A)}$

--

$P(B|A) = \frac{P(B)L(B|A)}{P(A)}$

--

Recall Law of Total Probability,     

$P(A) = P(A\cap B) + P(A\cap B^c)$

$P(A) = P(A|B)P(B) + P(A|B^c)P(B^c)$

---

class: middle

.pull-left[
$P(B|A) = \frac{P(B)L(B|A)}{P(A|B) P(B)+P(A|B^c) P(B^c)}$
]

--
.pull-right30[

$P(B) = 0.40$

$P(A|B) = 0.20$

$P(A|B^c) = 0.05$

]

--

$P(B|A) = \frac{0.40 \cdot 0.20}{(0.20 \cdot 0.40) + (0.05 \cdot 0.60)}$

---

class: middle

### The Posterior Model
<div align="center">

| event                 | $B$  | $B^c$ | Total |
|-----------------------|------|-------|-------|
| prior probability     | 0.4  | 0.6   | 1     |
| posterior probability | 0.72 | 0.18  | 1     |


---

class: middle

<div align="center">

## Likelihood is not a probability distribution


| event                 | $B$  | $B^c$ | Total |
|-----------------------|------|-------|-------|
| prior probability     | 0.4  | 0.6   | 1     |
| likelihood            | 0.20 | 0.05  | 0.25  |
| posterior probability | 0.72 | 0.18  | 1     |



---

class: middle

# Summary

$$P(B |A) = \frac{P(B)L(B|A)}{P(A)}$$
--

$$\text{posterior} = \frac{\text{prior}\cdot\text{likelihood}}{\text{marginal probability}}$$

--

$$\text{posterior} = \frac{\text{prior}\cdot\text{likelihood}}{\text{normalizing constant}}$$

---



```{r echo = FALSE, message = FALSE}
library(tidyverse)
library(xaringanExtra)
```



## Notation

- We will use _Greek letters_ (eg: $\pi, \beta, \mu$) to denote our primary variables of interest.

- We will use _capital letters_ toward the end of the alphabet (eg: $X, Y, Z$) to denote random variables related to our data.  

- We denote an observed _outcome_ of $Y$ (a constant) using lower case $y$.  

---

# Review: Discrete probability models

Let $Y$ be a discrete random variable with probability mass function (pmf) $f(y)$.  Then the pmf defines the probability of any given $y$, $f(y) = P(Y = y)$, and has the following properties:    

$\sum_{\text{all } y} f(y) = 1$  

$0 \le f(y) \le 1$ for all values of $y$ in the range of $Y$



---

## PhD admissions

Let Y represent a random variable that represents the number of applicants admitted to a PhD program which has received applications from 5 prospective students. That is $\Omega_Y = \{0, 1, 2, 3, 4, 5\}$. We are interested in the parameter $\pi$ which represents the probability of acceptance to this program. For demonstrative purposes, we will only consider three possible values of $\pi$ as 0.2, 0.4, and 0.8. 

---

# Prior model for $\pi$

You are now a true Bayesian and decide to consult with an expert who knows the specific PhD program well and the following is the prior distribution the expert suggests you use in your analysis. 

<table align = "center">
<tr>
  <th> &pi;</th>
  <th> 0.2</th>
  <th> 0.4</th>
  <th> 0.8</th>

</tr>

<tr>
  <td> f(&pi;)</td>
  <td> 0.7</td>
  <td> 0.2</td>
  <td> 0.1</td>

</tr>
</table>

The expert thinks that this is quite a hard-to-get-into program.

---

# From prior to posterior

We have a prior model for $\pi$ that is $f(\pi)$. 

In light of observed data $y$ we can update our ideas about $\pi$. 

We will call this the posterior model $f(\pi|y)$.  

In order to do this update we will need data which we have not observed yet. 

---

## Consider data

For the two scenarios below fill out the table (twice). For now, it is OK to use your intuition to guesstimate. 

<table align = "center">
<tr>
  <th></th>
  <th> &pi;</th>
  <th> 0.2</th>
  <th> 0.4</th>
  <th> 0.8</th>

</tr>

<tr>
  <td></td>
  <td> f(&pi;)</td>
  <td> 0.7</td>
  <td> 0.2</td>
  <td> 0.1</td>

</tr>

<tr>
  <td>Scenario 1</td>
  <td> f(&pi;|y)</td>
  <td> </td>
  <td> </td>
  <td> </td>

</tr>

<tr>
  <td> Scenario 2</td>
  <td> f(&pi;|y)</td>
  <td> </td>
  <td> </td>
  <td> </td>

</tr>



</table>

Scenario 1: What if this program accepted five of the five applicants?

Scenario 2: What if this program accepted none of the five applicants? 
---

## Intuition vs. Reality

Your intuition may not be Bayesian if
- you have only relied on the prior model to decide on the posterior model.
- you have only relied on the data to decide on the posterior model.

Bayesian statistics is a balancing act and we will take both the prior and the data to get to the posterior. Don't worry if your intuition was wrong. As we practice more, you will learn to think like a Bayesian.

---

## Likelihood

We do not know $\pi$ but for now let's consider one of the three possibilities for $\pi = 0.2$. If $\pi$ were 0.2 what is the probability that we would observe 4 of the 5 applicants get admitted to the program? Would you expect this probability to be high or low?


--

Can you calculate an exact value?

---

## The Binomial Model

Let random variable $Y$ be the _number of successes_ (eg: number of accepted applicants) in $n$ _trials_ (eg: applications).  Assume that the number of trials is _fixed_, the trials are _independent_, and the _probability of success_ (eg: probability of acceptance) in each trial is $\pi$.  Then the _dependence_ of $Y$ on $\pi$ can be modeled by the Binomial model with __parameters__ $n$ and $\pi$.  In mathematical notation:
 
$$Y | \pi \sim \text{Bin}(n,\pi) $$

then, the Binomial model is specified by a conditional pmf:    

$$f(y|\pi) = {n \choose y} \pi^y (1-\pi)^{n-y} \;\; \text{ for } y \in \{0,1,2,\ldots,n\}$$

---

## The Binomial Model

$f(y = 4 | \pi = 0.2) = {5 \choose 4} 0.2^40.8^1 = \frac{5!}{(5-4)! 4!} 0.2^40.8^1= 0.0064$  

or using R

```{r echo = TRUE}
dbinom(x = 4, size = 5, prob = 0.2)
```

---

## The Binomial Model

If $\pi$ were 0.2 what is the probability that we would observe 3 of the 5 applicants get admitted to the program? Would you expect this probability to be high or low?

$f(x = 3 | \pi = 0.2) = {5 \choose 3} 0.2^30.8^2 = \frac{5!}{(5-3)! 3!} 0.2^30.8^2 =0.0512$  

or using R

```{r echo = TRUE}
dbinom(x = 3, size = 5, prob = 0.2)
```

---

## The Binomial Model

Rather than doing this one-by-one we can let R consider all different possible observations of y, 0 through 5. 

```{r}
dbinom(x = 0:5, size = 5, prob = 0.2)
```


---

## Probabilities for $y_is$ if $\pi = 0.2$

```{r echo = FALSE, fig.align = 'center'}
# Set up plot data
n   <- 5
pi  <- c(0.2, 0.4, 0.8)
pis <- data.frame(setting = factor(rep(1:length(pi), each = (n + 1))),
    x = rep(0:n, length(pi)),
    pi = rep(pi, each = (n + 1)))
pis <- pis %>% 
    mutate(y = dbinom(x, size = n, prob = pi)) %>% 
    mutate(x_observed = as.factor(x == 3))
levels(pis$setting) <- paste0("Bin(",n,", ",pi,")")
pis_1 <-pis %>% 
  filter(pi == 0.2) 
  
 
ggplot(pis_1, aes(x = x, y = y)) + 
    lims(x = c(0,n), y = c(0, 0.6)) + 
    geom_point(size = 0.75) + 
    geom_segment(data = pis_1, 
                 aes(x = x, 
                     y = rep(0,length(y)), 
                     xend = x, 
                     yend = y, 
                     )) +
    labs(x = "y", y = expression(paste("f(y|",pi,")"))) + 
    scale_color_manual(values = c("black","red")) + 
    theme(legend.position="none") 
``` 


---

## Other possibilities for $\pi$

```{r echo = FALSE, fig.align = 'center'}
ggplot(pis, aes(x = x, y = y)) + 
    lims(x = c(0,n), y = c(0, max(pis$y))) + 
    geom_point(size = 0.75) + 
    facet_wrap(~ setting) + 
    geom_segment(data = pis, aes(x = x, y = rep(0,length(y)), xend = x, yend = y)) +
    labs(x = "y", y = expression(paste("f(y|",pi,")"))) + 
    scale_color_manual(values = c("black","red")) + 
    theme(legend.position="none") 
```

---

## Data

The admissions committee has announced that they have accepted 3 of the 5 applicants. 

---

## Data

```{r echo = FALSE, fig.align='center'}
ggplot(pis, aes(x = x, y = y)) + 
    lims(x = c(0,n), y = c(0, max(pis$y))) + 
    geom_point(size = 0.75, aes(color = x_observed)) + 
    facet_wrap(~ setting) + 
    geom_segment(data = pis, aes(x = x, y = rep(0,length(y)), xend = x, yend = y, color = x_observed)) +
    labs(x = "y", y = expression(paste("f(y|",pi,")"))) + 
    scale_color_manual(values = c("black","red")) + 
    theme(legend.position="none") 
```

---

## Likelihood


```{r echo = FALSE, fig.align='center'}
just_3 <- pis %>% 
  filter(x == 3)
ggplot(just_3, aes(x = pi, y = y)) + 
  geom_point(size = 0.75, aes(color = "red")) + 
  geom_segment(data = just_3, aes(x = pi, y = rep(0,length(y)), xend = pi, yend = y, color = "red")) +
  labs(x = expression(pi), y = expression(paste("L(",pi,"|(x=3))"))) + 
  theme(legend.position="none") +
  scale_x_continuous(breaks = c(0.2, 0.4, 0.8))
```

---

## Likelihood

```{r}
dbinom(x = 3, size = 5, prob = 0.2)
```

```{r}
dbinom(x = 3, size = 5, prob = 0.4)
```

```{r}
dbinom(x = 3, size = 5, prob = 0.8)
```

---

## Likelihood

<table align = "center">
<tr>
  <th> &pi; </th>
  <th> 0.2</th>
  <th> 0.4</th>
  <th> 0.8</th>

</tr>

<tr>
  <td> L(&pi; | y = 3)</td>
  <td> 0.0512</td>
  <td> 0.2304</td>
  <td> 0.2048</td>

</tr>


</table>


---

## Likelihood

The likelihood function $L(\pi|y=3)$ is the same as the conditional probability mass function $f(y|pi)$ at the observed value $y = 3$.

---

### __pmf vs likelihood__    

When $\pi$ is known, the __conditional pmf__ $f(\cdot | \pi)$ allows us to compare the probabilities of different possible values of data $Y$ (eg: $y_1$ or $y_2$) occurring with $\pi$: 

$$f(y_1|\pi) \; \text{ vs } \; f(y_2|\pi) \; .$$  

When $Y=y$ is known, the __likelihood function__ $L(\cdot | y) = f(y | \cdot)$ allows us to compare the relative likelihoods of different possible values of $\pi$ (eg: $\pi_1$ or $\pi_2$) given that we observed data $y$:

$$L(\pi_1|y) \; \text{ vs } \; L(\pi_2|y) \; .$$

---



## Getting closer to conclusion

The expert assigned the highest weight to $\pi = 0.2$. 

However the data $y = 3$ suggests that $\pi = 0.4$ is more likely. 

We will continue to consider all the possible values of $\pi$.

Now is a good time to balance the prior and the likelihood.

---

## From events to random variables

$\text{posterior} = \frac{\text{prior} \times \text{likelihood}}{\text{marginal probability of data}}$

--

$\text{posterior} = \frac{\text{prior} \times \text{likelihood}}{f(y = 3)}$

--

$\text{posterior} = \frac{\text{prior} \times \text{likelihood}}{f(y = 3 \cap \pi = 0.2) + f(y = 3 \cap \pi = 0.4) + f(y = 3 \cap \pi = 0.8)}$

--

$\text{posterior} = \frac{\text{prior} \times \text{likelihood}}{f(y = 3 | \pi = 0.2) \cdot (\pi = 0.2) + f(y = 3 | \pi = 0.4) \cdot (\pi = 0.4) + f(y = 3 | \pi = 0.8) \cdot (\pi = 0.8)}$


---

## Normalizing constant


$\text{posterior} = \frac{\text{prior} \times \text{likelihood}}{f(y = 3 | \pi = 0.2) \cdot (\pi = 0.2) + f(y = 3 | \pi = 0.4) \cdot (\pi = 0.4) + f(y = 3 | \pi = 0.8) \cdot (\pi = 0.8)}$


Thus $f(y = 3) =$

```{r}
dbinom(x = 3, size = 5, prob = 0.2) * 0.7 +
dbinom(x = 3, size = 5, prob = 0.4) * 0.2 +
dbinom(x = 3, size = 5, prob = 0.8) * 0.1
```

---

# Posterior 

<table align = "center">
<tr>
  <th> &pi; </th>
  <th> 0.2</th>
  <th> 0.4</th>
  <th> 0.8</th>

</tr>

<tr>
  <td> f(&pi;)</td>
  <td> 0.7</td>
  <td> 0.2</td>
  <td> 0.1</td>

</tr>

<tr>
  <td> L(&pi; | y = 3)</td>
  <td> 0.0512</td>
  <td> 0.2304</td>
  <td> 0.2048</td>

</tr>


<tr>
  <td> f(&pi; | y = 3)</td>
  <td> </td>
  <td> </td>
  <td> </td>

</tr>

</table>

--

$f(\pi=0.2 | y = 3) = \frac{f(\pi)L(\pi|y =3)}{f(y = 3)}$

--

$= \frac{0.7 \times 0.0512}{0.1024}$

--

$= 0.35$

---

# Posterior 

<table align = "center">
<tr>
  <th> &pi; </th>
  <th> 0.2</th>
  <th> 0.4</th>
  <th> 0.8</th>

</tr>

<tr>
  <td> f(&pi;)</td>
  <td> 0.7</td>
  <td> 0.2</td>
  <td> 0.1</td>

</tr>

<tr>
  <td> L(&pi; | y = 3)</td>
  <td> 0.0512</td>
  <td> 0.2304</td>
  <td> 0.2048</td>

</tr>


<tr>
  <td> f(&pi; | y = 3)</td>
  <td>0.35</td>
  <td> </td>
  <td> </td>

</tr>

</table>

--

$f(\pi=0.4 | y = 3) = \frac{f(\pi)L(\pi|y =3)}{f(y = 3)}$

--

$= \frac{0.2 \times 0.2304}{0.1024}$

--

$= 0.45$


---

# Posterior 

<table align = "center">
<tr>
  <th> &pi; </th>
  <th> 0.2</th>
  <th> 0.4</th>
  <th> 0.8</th>

</tr>

<tr>
  <td> f(&pi;)</td>
  <td> 0.7</td>
  <td> 0.2</td>
  <td> 0.1</td>

</tr>

<tr>
  <td> L(&pi; | y = 3)</td>
  <td> 0.0512</td>
  <td> 0.2304</td>
  <td> 0.2048</td>

</tr>


<tr>
  <td> f(&pi; | y = 3)</td>
  <td>0.35</td>
  <td>0.45</td>
  <td> </td>

</tr>

</table>

--

$f(\pi=0.8 | y = 3) = \frac{f(\pi)L(\pi|y =3)}{f(y = 3)}$

--

$= \frac{0.1 \times 0.2048}{0.1024}$

--

$= 0.2$

---

# Posterior 

<table align = "center">
<tr>
  <th> &pi; </th>
  <th> 0.2</th>
  <th> 0.4</th>
  <th> 0.8</th>

</tr>

<tr>
  <td> f(&pi;)</td>
  <td> 0.7</td>
  <td> 0.2</td>
  <td> 0.1</td>

</tr>

<tr>
  <td> L(&pi; | y = 3)</td>
  <td> 0.0512</td>
  <td> 0.2304</td>
  <td> 0.2048</td>

</tr>


<tr>
  <td> f(&pi; | y = 3)</td>
  <td>0.35</td>
  <td>0.45</td>
  <td>0.2 </td>

</tr>

</table>


---

### Why is normalizing constant a "normalizing constant"?


.panelset[

.panel[.panel-name[&pi; = 0.2]

$f(\pi=0.2 | y = 3) = \frac{f(\pi)L(\pi|y =3)}{f(y = 3)}$

$= \frac{0.7 \times 0.0512}{0.1024}$

]

.panel[.panel-name[&pi; = 0.4]

$f(\pi=0.4 | y = 3) = \frac{f(\pi)L(\pi|y =3)}{f(y = 3)}$

$= \frac{0.2 \times 0.2304}{0.1024}$


]


.panel[.panel-name[&pi; = 0.8]

$f(\pi=0.8 | y = 3) = \frac{f(\pi)L(\pi|y =3)}{f(y = 3)}$

$= \frac{0.1 \times 0.2048}{0.1024}$

]

]

---

### Why is normalizing constant a "normalizing constant"?



.panelset[

.panel[.panel-name[&pi; = 0.2]

$f(\pi=0.2 | y = 3) = \frac{f(\pi)L(\pi|y =3)}{f(y = 3)}$

$= \frac{0.7 \times 0.0512}{0.1024}$

$\propto {0.7 \times 0.0512}$

]

.panel[.panel-name[&pi; = 0.4]

$f(\pi=0.4 | y = 3) = \frac{f(\pi)L(\pi|y =3)}{f(y = 3)}$

$= \frac{0.2 \times 0.2304}{0.1024}$


$\propto 0.2 \times 0.2304$


]


.panel[.panel-name[&pi; = 0.8]

$f(\pi=0.8 | y = 3) = \frac{f(\pi)L(\pi|y =3)}{f(y = 3)}$

$= \frac{0.1 \times 0.2048}{0.1024}$

$\propto 0.1 \times 0.2048$

]

]

---

class: center middle

$$f(\pi|y) \propto f(\pi)L(\pi|y)$$


---
## In summary

Every Bayesian analysis consists of three common steps.   

1.Construct a __prior model__ for your variable of interest, $\pi$.    
    A prior model specifies two important pieces of information: the possible values of $\pi$ and the relative prior plausibility of each.  

2.Upon observing data $Y = y$, define the __likelihood function__ $L(\pi|y)$.    
    As a first step, we summarize the dependence of $Y$ on $\pi$ via a __conditional pmf__ $f(y|\pi)$.  The likelihood function is then defined by $L(\pi|y) = f(y|\pi)$ and can be used to compare the relative likelihood of different $\pi$ values in light of data $Y = y$.

---

## In summary

3.Build the __posterior model__ of $\pi$ via Bayes' Rule.    
    By Bayes' Rule, the posterior model is constructed by balancing the prior and likelihood:

$$\text{posterior} = \frac{\text{prior} \cdot \text{likelihood}}{\text{normalizing constant}} \propto \text{prior} \cdot \text{likelihood}$$
More technically,
    
$$f(\pi|y) = \frac{f(\pi)L(\pi|y)}{f(y)} \propto f(\pi)L(\pi|y)$$




